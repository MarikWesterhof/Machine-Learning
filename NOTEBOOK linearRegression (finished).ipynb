{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFkCAYAAADFZ4k9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c75ad9550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045 0.0455980515489\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    h0 = theta0 + (theta1*x)\n",
    "    inner = power(((h0.T) - y), 2) #Transposing is not necessary\n",
    "    return sum(inner)/(2*len(x))\n",
    "    \n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    #your code here\n",
    "    h0 = theta0 + (theta1*x)\n",
    "    inner = (h0 - y)\n",
    "    t0 = theta0 - learningrate*((1/len(x))*sum(1*inner))\n",
    "    t1 = theta1 - learningrate*((1/len(x))*sum(x*inner))\n",
    "    theta0 = t0 # assigned new values\n",
    "    theta1 = t1 # with temp values\n",
    "    print( inner)\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.94352738 -2.03618875 -1.85297007 -1.89043714 -2.26464503 -2.22790161\n",
      " -2.37356172 -2.05392546 -3.00611136 -2.48095076 -2.97626226 -2.71408378\n",
      " -2.53899826 -3.36877359 -2.85364466 -2.82711419 -3.13297991 -2.9580719\n",
      " -2.8394688  -2.97366061 -2.81593849 -2.98565634 -3.80018005 -2.88726643\n",
      " -2.69314592 -3.03479085 -2.98429327 -3.20946938 -3.33897299 -3.28293867\n",
      " -3.64469304 -3.53552654 -3.8769354  -4.27766794 -3.51443047 -4.14724999\n",
      " -3.47940651 -3.55781003 -4.11545695 -3.32898109 -4.23756278 -3.91995864\n",
      " -4.15789722 -4.41189312 -4.12217229 -3.82382781 -4.29671571 -4.32527442\n",
      " -5.04786512 -4.14973471 -4.66316982 -3.8965497  -4.96216963 -4.90227026\n",
      " -4.61101068 -4.88102067 -5.03003803 -5.2299917  -4.48434358 -4.73636412\n",
      " -4.81695708 -5.38601178 -5.26549156 -4.83331479 -5.39226011 -5.52307415\n",
      " -5.93151306 -5.73123111 -5.37524975 -5.80051505 -5.7929604  -5.76791703\n",
      " -5.95021277 -5.55757428 -6.13185726 -5.53001501 -5.91276226 -5.88783453\n",
      " -5.54668585 -6.16006307 -5.89807763 -5.37707946 -6.34571893 -6.0834814\n",
      " -5.82114361 -6.31675665 -6.47287268 -6.34634929 -6.38929764 -6.52104237\n",
      " -6.03893878 -6.74240349 -6.63351291 -6.65409984 -6.29506674 -6.61137204\n",
      " -7.16134102 -6.6670074  -7.45906927 -7.13157902]\n",
      "11.254682284 >= 5.15358394677\n"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
